{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1- Neural Networks Basics with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Learning PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 5\n",
      " 8\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n",
      "\n",
      " 2\n",
      " 5\n",
      " 8\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n",
      "\n",
      " 2\n",
      " 5\n",
      " 8\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part (a)\n",
    "t = torch.Tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "col1 = t[:, 1:2]\n",
    "col2 = torch.split(t,1,1)[1]\n",
    "col3 = t.select(1,1)\n",
    "\n",
    "print(col1)\n",
    "print(col2)\n",
    "print(col3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b)\n",
    "\n",
    "#### Tensor: A PyTorch Tensor is basically the same as a numpy array. The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU, just cast the Tensor to a cuda datatype.\n",
    "\n",
    "#### Variable: PyTorch variable is a wrapper around the tensors, and represent  a node in computational graph. Variable has multiple operations defined on underlying tensor.\n",
    "\n",
    "#### Storage: Storage is a contigous, one-dimensional array of a single data type. Every tensor has a corresponding storage of the same data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Regression with linear neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment for part (a):\n",
    "# POLY_DEGREE = 4\n",
    "# W_target = torch.randn(POLY_DEGREE, 1).cuda() * 5\n",
    "# b_target = torch.randn(1).cuda() * 5\n",
    "\n",
    "# print(W_target)\n",
    "# print(b_target)\n",
    "\n",
    "# Uncomment for part (b):\n",
    "data = pd.read_csv(\"./qn2_data.csv\", delimiter = ',', names = ['fertilizers', 'insecticides', 'production'])\n",
    "train_x = data.drop('production', 1).as_matrix()\n",
    "train_y = data['production'].as_matrix()\n",
    "train_y = train_y.reshape((train_y.shape[0],1))\n",
    "\n",
    "train_x = torch.from_numpy(train_x).cuda().float()\n",
    "train_y = torch.from_numpy(train_y).cuda().float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_features(x):\n",
    "#     Builds features i.e. a matrix with columns [x x^2 x^3 x^4].\n",
    "    x = x.unsqueeze(1)\n",
    "    return torch.cat([x**i for i in range(1, POLY_DEGREE+1)], 1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "#     Approximated function\n",
    "    return x.mm(W_target) + b_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def poly_desc(W, b):\n",
    "#     Creates a string description of a polynomial\n",
    "    result = 'y = '\n",
    "    for i ,w in enumerate(W):\n",
    "        result += '{:+.2f} x^{} '.format(w, len(W) - 1)\n",
    "    result += '{:+.2f}'.format(b[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size = 32):\n",
    "#     Builds a batch i.e. (x, f(x)) pair.\n",
    "    random = torch.randn(batch_size).cuda()\n",
    "    x = make_features(random)\n",
    "    y = f(x)\n",
    "    return Variable(x), Variable(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_dataset():# Define Model\n",
    "    print(W_target.size(0))\n",
    "    fc = torch.nn.Linear(W_target.size(0), 1).cuda()\n",
    "    lr_rate = [0.01,0.025,0.05,0.1,0.2]\n",
    "    for learning_rate in lr_rate:\n",
    "        optimizer= torch.optim.SGD(fc.parameters(), lr = learning_rate)\n",
    "\n",
    "        for batch_idx in count(1):\n",
    "        #     Get data\n",
    "            batch_x, batch_y = get_batch()\n",
    "#             print(batch_x)\n",
    "#             print(batch_y)\n",
    "\n",
    "        #     Reset Gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        #     Forward pass\n",
    "            output = F.smooth_l1_loss(fc(batch_x), batch_y)\n",
    "            loss = output.data[0]\n",
    "\n",
    "        #     Backward Pass\n",
    "            output.backward()\n",
    "\n",
    "#         #     Apply Gradients\n",
    "#             for param in fc.parameters():\n",
    "#                 param.data.add_(-0.1 * param.grad.data)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        #     Stop Criterion\n",
    "            if(loss < 1e-3):\n",
    "                break\n",
    "        print('Learning Rate-> {:+.3f}'.format(learning_rate))\n",
    "        print('Loss: {:+.6f} after {} batches'.format(loss, batch_idx))\n",
    "        print('==> Learned function:\\t' + poly_desc(fc.weight.data.view(-1), fc.bias.data))\n",
    "        print('==> Actual function:\\t' + poly_desc(W_target.view(-1), b_target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Learning rate on loss and batch size\n",
    "\n",
    "##### Learning Rate-> +0.010\n",
    "###### Loss: +0.000957 after 316 batches\n",
    "\n",
    "##### Learning Rate-> +0.025\n",
    "###### Loss: +0.000568 after 811 batches\n",
    "\n",
    "##### Learning Rate-> +0.050\n",
    "###### Loss: +0.000531 after 2431 batches\n",
    "\n",
    "##### Learning Rate-> +0.100\n",
    "###### Loss: +0.000855 after 4039 batches\n",
    "\n",
    "##### Learning Rate-> +0.200\n",
    "###### Loss: +0.000486 after 26370 batches\n",
    "\n",
    "###### As we can see from above results, as we increase the learning rate, number of batches required to converge increases(more or less).\n",
    "##### While in the example given below we can see that as the learning rate increases number of batches required decreases. So we can conclude that it actually depends on the function we are trying to learn. As we increase the learning rate the chances of overshooting from the solution increases, the tipping learning rate diffres for every fucntion, that is the reason we get different results for different fucntions.\n",
    "\n",
    "##### Learning Rate-> +0.010\n",
    "###### Loss: +0.000982 after 2327 batches\n",
    "\n",
    "##### Learning Rate-> +0.025\n",
    "###### Loss: +0.000817 after 14 batches\n",
    "\n",
    "##### Learning Rate-> +0.050\n",
    "###### Loss: +0.000866 after 1 batches\n",
    "\n",
    "##### Learning Rate-> +0.100\n",
    "###### Loss: +0.000490 after 53 batches\n",
    "\n",
    "##### Learning Rate-> +0.200\n",
    "###### Loss: +0.000526 after 1 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_dataset():\n",
    "    fc = torch.nn.Linear(train_x.size(1), 1).cuda()\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    optimizer= torch.optim.SGD(fc.parameters(), lr = learning_rate, momentum = 0.9, nesterov = True)\n",
    "\n",
    "    for epoch in count(1):\n",
    "        #     Get data\n",
    "        batch_x, batch_y = Variable(train_x), Variable(train_y)\n",
    "        #     Reset Gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #     Forward pass\n",
    "        output = F.smooth_l1_loss(fc(batch_x), batch_y)\n",
    "        loss = output.data[0]\n",
    "\n",
    "        #     Backward Pass\n",
    "        output.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        #     Stop Criterion\n",
    "        if(loss < 0.59):\n",
    "            break\n",
    "    print('Learning Rate-> {:+.3f}'.format(learning_rate))\n",
    "    print('Loss: {:+.6f} after {} batches'.format(loss, epoch))\n",
    "#     print('==> Learned function:\\t' + poly_desc(fc.weight.data.view(-1), fc.bias.data))\n",
    "#     print('==> Actual function:\\t' + poly_desc(W_target.view(-1), b_target))\n",
    "    return learning_rate, epoch, fc.weight.data.view(-1), fc.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate-> +0.001\n",
      "Loss: +0.589994 after 20773 batches\n"
     ]
    }
   ],
   "source": [
    "lr, epoch, weight, bias = toy_dataset()\n",
    "\n",
    "test = torch.Tensor([[6,4],[10,5],[14,8]]).cuda()\n",
    "ans = test.mm(weight.view(-1,1)) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Learing rate -> 0.001, Nesterov = True, momentum = 0.9\n",
    "#### Loss -> +0.589992 after 20599 batches\n",
    "#### Weights = [0.7674, 0.9943]\n",
    "#### Bias = [31.3090]\n",
    "#### Answer for test data = [39.8905, 43.9544, 50.0068] ==> [40, 44, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square():\n",
    "    bias = np.ones((train_x.shape[0],train_x.shape[1]+1))\n",
    "\n",
    "    bias[:, 1:] = train_x\n",
    "    X = torch.from_numpy(bias).float().cuda()\n",
    "    y = train_y\n",
    "\n",
    "    \n",
    "    theta = torch.mm(torch.mm((torch.inverse(torch.mm(torch.t(X),X).cuda()).cuda()), torch.t(X)), y).cuda()\n",
    "    \n",
    "    weight_closed = theta[1:3, :]\n",
    "    bias_closed = theta[0, :]\n",
    "\n",
    "    ans = test.mm(weight_closed) + bias_closed\n",
    "least_square()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights = [0.6500, 1.1099]\n",
    "#### Bias = [31.9806]\n",
    "#### Answer for test data = [ 40.3203, 44.0302, 49.9599] ==> [40, 44, 50]\n",
    "\n",
    "#### As we can see weights and bias terms are comparable to those of linear neuron. And the approximate predictions are same as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: MNIST Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = 0.9, nesterov = True)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        model_out = model(data)\n",
    "        loss = F.nll_loss(model_out, target)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    # TODO: Test the model on the test-set and report the loss and accuracy.\n",
    "    test_loss = 0\n",
    "    total_correct = 0\n",
    "    total_incorrect = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data,target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        model_out = model(data)\n",
    "        \n",
    "        # sum up batch loss\n",
    "        curr_loss = F.nll_loss(model_out,target).data[0]\n",
    "        test_loss += curr_loss\n",
    "        \n",
    "        predictions = model_out.data.max(1)[1]\n",
    "    \n",
    "        # code to compute classification error\n",
    "        correct = predictions.eq(target.data).sum()\n",
    "        total_correct += correct\n",
    "        total_incorrect += len(target.data) - correct\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, total_correct, len(test_loader.dataset),\n",
    "        100. * total_correct / len(test_loader.dataset)))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-> 1\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.319717\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.272489\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.267817\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.239270\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.021072\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 1.660417\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 1.396925\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 1.240363\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.146855\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.007413\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.822439\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.936733\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.013920\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.831634\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.480980\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.513170\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.790009\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.454616\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.489285\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.648589\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.616768\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.544563\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.756695\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.632253\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.510551\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.454492\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.514776\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.761293\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.498118\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.567577\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.338030\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.402944\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.431308\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.514872\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.551828\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.559588\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.578602\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.516555\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.261938\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.837799\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.562525\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.364367\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.445816\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.443834\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.266720\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.690762\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.379965\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.402376\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.610453\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.291387\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.412211\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.314513\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.314252\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.269382\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.314571\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.503196\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.362947\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.344931\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.290959\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.354299\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.583644\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.373207\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.229017\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.263468\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.215653\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.205627\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.325283\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.368134\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.421288\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.374100\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.600722\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.414882\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.475177\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.365003\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.375598\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.242915\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.199122\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.376513\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.326364\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.229568\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.470113\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.261337\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.363494\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.425778\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.292298\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.394785\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.395746\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.207007\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.354628\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.301709\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.305333\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.328970\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.371276\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.534015\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 9082/10000 (91%)\n",
      "\n",
      "Epoch-> 2\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.342375\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.136338\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.195252\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.566742\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.253574\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.354485\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.344129\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.104092\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.212213\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.102559\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.226106\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.230986\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.286501\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.201490\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.226864\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.368343\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.252216\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.316786\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.330373\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.315033\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.236924\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.162304\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.199590\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.168470\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.164880\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.269857\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.210600\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.208624\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.358602\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.292764\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.329427\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.307346\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.216328\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.155986\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.158404\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.160828\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.142355\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.186331\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.353818\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.231920\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.260499\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.203839\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.278228\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.223254\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.290038\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.117887\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.307852\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.222935\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.292611\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.216811\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.244962\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.229214\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.452742\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.179279\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.231786\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.287426\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.280380\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.100491\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.427081\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.204688\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.126441\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.121843\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.400795\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.177437\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.187551\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.118076\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.275318\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.303340\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.235639\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.482330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.195514\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.543191\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.429723\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.302592\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.285382\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.310178\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.302933\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.297159\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.113886\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.349918\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.248360\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.308601\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.155287\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.407418\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.202357\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.320460\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.215749\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.142953\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.216939\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.173944\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.443696\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.185034\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.328764\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.249442\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 9301/10000 (93%)\n",
      "\n",
      "Epoch-> 3\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.177367\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.247655\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.213098\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.152817\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.191199\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.365895\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.119430\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.257497\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.208720\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.652993\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.232430\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.348723\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.162538\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.182428\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.179704\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.446029\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.309904\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.125209\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.350517\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.487825\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.173333\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.057758\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.129209\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.150796\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.167953\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.230365\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.172359\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.128342\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.099097\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.202174\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.368067\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.053153\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.266729\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.395914\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.216406\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.240038\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.264100\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.438061\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.268808\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.323886\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.139290\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.312552\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.144860\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.082440\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.161529\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.587587\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.444914\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.372061\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.257350\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.222841\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.234329\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.117360\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.119744\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.086590\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.040684\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.181132\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.129475\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.123079\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.274436\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.322803\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.146907\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.342718\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.229987\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.320108\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.079237\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.266265\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.092863\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.242656\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.218860\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.172240\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.185800\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.152649\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.314724\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.132292\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.241173\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.233128\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.106264\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.379516\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.221471\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.095604\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.609216\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.293516\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.195835\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.211244\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.327954\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.146920\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.219545\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.118084\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.262156\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.132922\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.083981\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.154177\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.158318\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.349362\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 9483/10000 (95%)\n",
      "\n",
      "Epoch-> 4\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.146039\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.186096\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.294966\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.031804\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.059566\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.464423\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.357880\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.267681\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.352614\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.118646\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.168336\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.174266\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.323396\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.105482\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.609010\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.310528\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.163508\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.156775\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.491230\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.099784\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.220009\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.138991\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.095577\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.258782\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.107661\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.212606\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.118758\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.301848\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.135417\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.242752\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.216571\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.167758\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.165220\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.233138\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.380856\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.115603\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.131182\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.306731\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.166476\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.065049\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.356622\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.176079\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.180957\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.234110\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.104016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.188588\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.172760\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.064840\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.124130\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.110402\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.328015\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.170706\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.122967\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.191054\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.300233\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.184452\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.232235\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.090073\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.118668\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.150328\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.222238\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.223536\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.215779\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.218885\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.090005\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.174923\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.122778\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.238528\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.124671\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.225231\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.424567\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.067605\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.217292\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.144632\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.054122\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.239807\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.092888\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.258238\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.076236\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.176315\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.240746\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.097977\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.119156\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.227337\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.185549\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.369416\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.144625\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.276737\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.139405\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.289918\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.072089\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.204829\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.221797\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.261988\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 9448/10000 (94%)\n",
      "\n",
      "Epoch-> 5\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.171717\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.275911\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.179900\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.072089\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.255515\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.113096\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.190783\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.185113\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.202841\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.425630\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.112357\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.546930\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.110244\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.093737\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.071321\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.288734\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.102787\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.071471\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.132735\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.199684\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.093392\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.169860\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.080693\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.080253\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.105970\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.111818\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.202980\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.163317\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.258682\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.180517\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.133174\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.105060\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.042743\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.124731\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.155727\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.261631\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.174460\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.173688\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.129302\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.107814\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.151924\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.129433\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.216036\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.163462\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.159623\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.217374\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.256041\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.162322\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.221611\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.335654\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.259193\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.110681\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.325226\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.415806\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.203047\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.055004\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.155139\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.107474\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.106876\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.231084\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.127998\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.148145\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.094636\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.123785\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.062072\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.121622\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.138015\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.329824\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.035361\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.227338\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.232000\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.101887\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.220147\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.134587\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.130008\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.197154\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.139800\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.074707\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.057214\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.190284\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.154524\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.177181\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.375059\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.114406\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.095544\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.088267\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.055436\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.189404\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.021768\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.319103\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.269769\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.250400\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.076979\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.061629\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 9542/10000 (95%)\n",
      "\n",
      "Epoch-> 6\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.060264\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.149471\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.095625\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.151812\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.195037\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.275868\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.129016\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.093071\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.073050\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.151991\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.114265\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.120761\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.147697\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.069017\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.171469\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.216209\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.203351\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.217658\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.057007\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.144578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.220061\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.279405\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.376673\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.172748\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.069780\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.116614\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.081559\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.611956\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.083814\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.240673\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.009640\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.221696\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.138240\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.056893\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.201859\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.228355\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.108320\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.100252\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.015008\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.118864\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.140660\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.182333\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.173663\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.137263\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.128063\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.185345\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.244506\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.245706\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.135344\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.163561\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.130426\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.136396\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.140644\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.114880\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.169093\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.067025\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.078464\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.053413\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.253875\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.222182\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.230744\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.283279\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.125393\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.104695\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.331984\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.063470\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.160923\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.136758\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.201123\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.072169\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.105225\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.228161\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.133284\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.162180\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.071879\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.199558\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.091736\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.185064\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.067628\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.050147\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.015023\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.203102\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.160815\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.255248\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.183272\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.263986\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.121211\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.257166\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.226755\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.079982\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.136077\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.060517\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.061887\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.094454\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 9536/10000 (95%)\n",
      "\n",
      "Epoch-> 7\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.083368\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.381454\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.179174\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.077302\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.107684\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.100251\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.196835\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.124049\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.093994\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.105282\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.178273\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.070400\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.091447\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.157977\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.141409\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.095311\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.043922\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.038557\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.143138\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.109659\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.094537\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.105030\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.213301\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.041708\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.059814\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.159960\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.133893\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.152629\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.030448\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.138769\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.253905\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.123202\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.123354\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.094672\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.144779\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.250417\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.104593\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.062515\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.043117\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.136098\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.105822\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.255982\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.181162\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.216561\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.156529\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.253338\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.168184\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.081765\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.164529\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.071318\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.065440\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.140083\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.333048\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.188338\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.193217\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.111163\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.085399\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.140818\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.234192\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.121583\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.111751\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.135247\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.075700\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.552826\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.289552\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.146492\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.135992\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.239327\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.151036\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.063995\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.069230\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.114344\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.103674\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.154039\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.095555\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.252754\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.094172\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.258949\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.173546\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.131671\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.110903\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.159301\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.062518\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.092602\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.235667\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.135426\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.260265\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.131713\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.089548\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.055557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.162730\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.109488\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.139612\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.354053\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 9595/10000 (96%)\n",
      "\n",
      "Epoch-> 8\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.083127\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.096423\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.141162\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.248785\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.127907\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.156353\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.056674\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.087710\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.454992\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.078905\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.117626\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.110713\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.243942\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.123334\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.074869\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.037061\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.175445\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.067049\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.132180\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.145931\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.184802\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.069882\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.164932\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.213043\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.135195\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.266938\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.149531\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.077593\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.144885\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.051722\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.180516\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.168497\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.051477\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.393182\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.136539\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.480697\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.163598\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.141470\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.117868\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.322923\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.274616\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.074817\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.140721\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.106729\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.098512\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.060003\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.359763\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.188993\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.132099\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.392692\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.171918\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.073176\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.041388\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.109533\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.154579\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.052815\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.108628\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.091538\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.151952\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.139214\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.086189\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.412608\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.147280\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.075607\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.180643\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.327995\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.291856\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.124793\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.126210\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.039808\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.208627\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.139390\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.050351\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.219325\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.089569\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.224766\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.047000\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.309582\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.290367\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.083867\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.204332\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.119563\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.327441\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.226281\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.115137\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.093482\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.034496\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.128918\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.157236\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.185956\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.179032\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.097064\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.113525\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.254845\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9557/10000 (96%)\n",
      "\n",
      "Epoch-> 9\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.132895\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.210546\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.130689\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.121793\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.093397\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.197884\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.077063\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.090100\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.061572\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.118213\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.142450\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.042728\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.120978\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.105557\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.193199\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.052272\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.029198\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.081357\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.016593\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.060556\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.115264\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.198088\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.271708\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.094556\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.137933\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.088687\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.071296\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.143573\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.373430\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.133553\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.069921\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.131933\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.086211\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.231053\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.240636\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.108687\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.212835\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.188550\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.083106\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.096509\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.179311\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.118613\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.073201\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.128702\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.111927\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.098006\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.182507\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.215061\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.052772\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.169239\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.205428\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.151741\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.154273\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.276571\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.281590\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.147525\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.135354\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.199826\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.176195\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.159763\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.139573\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.065793\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.147234\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.051895\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.098297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.049914\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.150517\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.102679\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.039367\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.186856\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.174496\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.153191\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.280312\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.161749\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.092386\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.120756\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.049655\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.170128\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.154828\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.119659\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.243131\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.070365\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.105025\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.228723\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.444237\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.149684\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.072434\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.228246\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.095301\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.068432\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.123896\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.225900\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.021710\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.142885\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9570/10000 (96%)\n",
      "\n",
      "Epoch-> 10\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.076291\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.143836\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.162397\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.233307\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.406032\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.061761\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.117346\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.119703\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.325614\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.059683\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.143524\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.380039\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.078845\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.046521\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.088017\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.071264\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.153791\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.157149\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.029156\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.217604\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.077031\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.092769\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.168791\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.083999\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.192929\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.240437\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.154329\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.193206\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.085236\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.241057\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.249561\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.091060\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.256772\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.078002\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.099174\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.040772\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.100425\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.040190\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.186237\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.162954\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.038184\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.377593\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.125172\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.206450\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.157989\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.099454\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.141477\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.036344\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.159937\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.075402\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.339867\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.164476\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.083264\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.064618\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.246801\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.084172\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.083536\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.131538\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.073360\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.230401\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.085201\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.143437\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.159420\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.161983\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.125906\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.328532\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.125449\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.186909\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.157350\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.239445\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.028788\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.169745\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.150653\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.139748\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.048811\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.181298\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.030266\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.282764\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.076158\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.167435\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.131553\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.116182\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.030726\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.260504\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.098203\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.053667\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.125259\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.069480\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.077041\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.253816\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.153783\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.107053\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.098651\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.111483\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 9502/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_arr = []\n",
    "test_arr = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print('Epoch-> {}'.format(epoch))\n",
    "    train_arr.append([train(epoch)])\n",
    "    test_arr.append([test()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEmCAYAAACqBQ3gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VPW9//HXJ3uAEHYie1RcCCpq0Kq4BBG0rbX3/hQX\nsNZq+WnrVWt7rUvdoHjV26tdbOnPqq0LitFu1C6oBbHqFQiKCyCIyr7vYAjZPr8/zkkYwiQkYTIz\nSd7Px2MeM+d7ts+MMu+c8z1zvubuiIiIxEJKogsQEZG2Q6EiIiIxo1AREZGYUaiIiEjMKFRERCRm\nFCoiIhIzChURiQszu9fM3MzOSXQt0nIUKtIiwi+Pgz3OSXSdrYk+U2kN0hJdgLR59zUwb3m8imhj\n9JlK0lKoSIty93sTXUNbo89UkplOf0lSiDzfbmZXmNkcM9ttZsvD+YPC+b8zs6PM7AUz22hm1ZGn\nfMxssJk9bWZrzKzczNaG04Obus8Gav043HaPeub/MNzuDRFtx5vZ82a23Mz2mtkmM3vXzH5qZunN\n/NgaVOf9XWVm75nZnvBze9LM8upZr9GfYbh8qpldZ2ZvmdmOcB/LzOzxBta52MzmmlmpmW01s2lm\n1jeW718SQ0cqkmy+D5wH/AWYBeTWmX8EMAdYCkwFsoGdAGY2HHgNyAGmA4uAY4DxwEVmNsrd5zVj\nn3U9BdwPXA78Isr8q4By4LmwruPDmj2s63OgM3Ak8B3gR0DFQfZ5KL4HjAZeAP4BjACuBs4xs1Pd\nfVPNgk39DM0sA3iZ4PNbRfCedwKDgH8D3gQ+qVPPd4CvhdufDZwKXAqcYGbD3H1vLN+8xJdCRVqU\nmd1bz6wyd38gSvtI4DR3f6+e9UYA/+Xud9TZjwFPE3xZj3f3qRHzLgWmAc+Y2RB3r27iPut6Bvgx\nQXjsFyrhl/KxwB/cfWvYfBWQBXzd3f9cZ/muQGkj91uzzr31zKrvM70AODXy/ZnZI8DNwAPANWFb\ncz7De9kXyJdEBoKZZYbbqut8YLi7fxix7HMEIX0RUFzvm5fk5+566BHzB8Ff5Q09ttdZ/t6w/ZF6\ntjconL8eyIwy/4xw/tv1rP+vcP5Zjd3nQd7fK+G6BXXaHw3bvxbR9j9h2+gEfaZPRNlWLrAd2FPz\neTb1MwRSw22UAn0aUX9NPT+OMq8onPeTRP+/q8ehPdSnIi3K3a2eR5d6Vpl7kE2+79FPj5wUPs+s\nZ72a9hObsc9ofhc+X1XTEJ4KuhzYCPwtYtkXgCrgT2HfxDfM7Ihm7BNo1mc6O8o2dgALCI6gjg2b\nm/oZHkMQTh+4+9omvIWSKG2rwueuTdiOJCGFiiSb9c2cX9MPsq6e+TXt0b54D7bPaP5I0Hcw3sxS\nw7avAt2A59y9smZBd58LnEnwpXwxQZ/MsrDD//Jm7LupNtTTXvO+c+s8N/YzrHle08R6tkdpq/m8\nUqPMk1ZEoSLJ5mCjxtU3f0f4HPWKJuCwOss1ZZ8HruC+h+Dc/2EEfQqw76jlqSjL/6+7f5XgL/Ez\ngElAb+A5MxvV1P03Ue962ms+qx11nhv7GdaEg67akloKFWkrajqhz6lnflH4/G4M9/m78PkqM+tJ\n0CH+gbsvqG8Fd9/r7m+7+93AjWHzRTGsKZqz6zaYWS4wDCgDFofNTf0MPyYIluPNrE9MKpVWT6Ei\nbcVbwBJghJldHDkjnD6T4DLkN2O1Q3d/i+By2YuA64B09gVN5P5PN7PsKJuoOYJo0tVfzXClmdXt\nS7qX4HTX8xF9VE36DN29CvgVwWXdvw6v9opcJyMMW2lHdEmxtKgGLn8F+FNDf9U3hbu7mV0FvAq8\nYGZ/JvhL+mjg68Au4Bt+4OXEh+ppglNZdxH0C0yNssytwEgz+xfBb1R2AwUERzbbgMeassNmfKZ/\nB94ys2KCfpER4WM5cFvNQs38DO8j+J3JhcBSM3s5XK4/wW9j/pMoQSttl0JFWto9DcxbTnAFUky4\n+5zwdyI/AkYRfNFtBp4HJrn7kljtK8LTBF+s6cDL7r4xyjK/IgiPUwm+zNOA1WH7/7j7iibus6mf\n6SMEFxbcTPAjw90EX/R31K23qZ+hu5eb2fkER2rfIOhXMmBtuM+YHRlK62DuTe6jFJFWIDyiuQco\ncvfXE1uNtBfqUxERkZhRqIiISMwoVEREJGbUpyIiIjGjIxUREYkZhYqIiMSMQkVERGJGoSIiIjGj\nUBERkZhRqIiISMwoVEREJGYUKiIiEjPt7i7FPXr08EGDBiW6DBGRVmP+/Pmb3b1RY+O0u1AZNGgQ\nJSUliS5DRKTVMLNGD8+g018iIhIzChUREYkZhYqIiMRMu+tTEZG2oaKigtWrV1NWVpboUtqMrKws\n+vXrR3p6erO3oVARkVZp9erV5OTkMGjQIMws0eW0eu7Oli1bWL16Nfn5+c3ejk5/NcbUqTBoEKSk\nBM9Tpya6IpF2r6ysjO7duytQYsTM6N69+yEf+elI5WCmToUJE6C0NJhesSKYBhg3LnF1iYgCJcZi\n8XnqSOVg7rxzX6DUKC0N2kVEZD8KlYNZubJp7SLSLmzZsoVhw4YxbNgw8vLy6Nu3b+10eXl5o7Zx\n9dVXs2TJkkbv8/HHH+fmm29ubslxodNfBzNgQHDKK1q7iLRb3bt3Z8GCBQDce++9dOrUiR/84Af7\nLePuuDspKdH/fv/tb3/b4nXGm45UDmbyZOjQYf+2Dh2CdhGROpYtW8aQIUMYN24cBQUFrFu3jgkT\nJlBYWEhBQQETJ06sXXbEiBEsWLCAyspKunTpwm233cYJJ5zAaaedxsaNGxu9z2effZbjjjuOoUOH\ncscddwBQWVnJlVdeWdv+85//HIBHHnmEIUOGcPzxxzN+/PjYvnl0pHJwYWe833EHvnIVO3rk0fWn\n/61OepEkct9fFrJo7c6YbnNIn87cc2FBs9b9+OOPefrppyksLATggQceoFu3blRWVlJUVMTFF1/M\nkCFD9ltnx44dnH322TzwwAPccsstPPnkk9x2220H3dfq1av50Y9+RElJCbm5uYwaNYqXX36Znj17\nsnnzZj788EMAtm/fDsBDDz3EihUryMjIqG2LJR2pNMa4cdiKFdz83HxG3fgUVZdfkeiKRCSJHXHE\nEbWBAvD8889z0kkncdJJJ7F48WIWLVp0wDrZ2dlccMEFAJx88sksX768UfuaM2cOI0eOpEePHqSn\np3PFFVfwxhtvcOSRR7JkyRJuvPFGZsyYQW5uLgAFBQWMHz+eqVOnHtKPHOujI5UmGFOQx/T311Ky\nfCunHt490eWISKi5RxQtpWPHjrWvP/nkE372s58xd+5cunTpwvjx46P+FiQjI6P2dWpqKpWVlYdU\nQ/fu3fnggw/4+9//zi9/+Ut+//vf89hjjzFjxgxmz57N9OnTuf/++/nggw9ITU09pH1F0pFKE5xz\ndE8y0lKYsXBDoksRkVZi586d5OTk0LlzZ9atW8eMGTNiuv1TTz2VWbNmsWXLFiorK5k2bRpnn302\nmzZtwt255JJLmDhxIu+++y5VVVWsXr2akSNH8tBDD7F582ZK6/5k4hDpSKUJOmamceaRPZixcD13\nffVY/fBKRA7qpJNOYsiQIRxzzDEMHDiQM84445C298QTT/DSSy/VTpeUlDBp0iTOOecc3J0LL7yQ\nr3zlK7z77rtcc801uDtmxoMPPkhlZSVXXHEFu3btorq6mh/84Afk5OQc6lvcj7l7TDeY7AoLC/1Q\nBukqnreKW3//AS//xwiG9s2NYWUi0hSLFy/m2GOPTXQZbU60z9XM5rt7YT2r7Eenv5ro3GN7kWLw\nysL1iS5FRCTpKFSaqHunTIYP6qZ+FRGRKBQqzTCmII8lG3bx+eYvEl2KiEhSUag0w+iC3gDM0Ckw\nEZH9KFSaoV/XDgzt21mhIiJSh0KlmcYMyeO9ldvZsFNDmYqI1FCoNNOYoXkAvLJIHfYi7VEsbn0P\n8OSTT7J+ffSzHuPHj+dPf/pTrEqOi7iFipmdb2ZLzGyZmR1wlzQzyzSzF8L5c8xsUMS828P2JWY2\nJqL9e2a20Mw+MrPnzSwrPu8GBvfqRH6Pjrq0WKSdqrn1/YIFC7juuuv43ve+VzsdecuVg2koVFqj\nuISKmaUCvwQuAIYAl5vZkDqLXQNsc/cjgUeAB8N1hwCXAQXA+cCvzCzVzPoCNwKF7j4USA2Xiwsz\nY3RBb/730y3sKK2I125FpBV46qmnOOWUUxg2bBjf+c53qK6ujnor+hdeeIEFCxZw6aWXNvoIp7q6\nmltuuYWhQ4dy3HHH1f66fs2aNYwYMYJhw4YxdOhQ3n777Xpvf9+S4nWbllOAZe7+GYCZTQMuAiJv\n1XkRcG/4+iXgUQvug3IRMM3d9wKfm9mycHsrCerPNrMKoAOwNg7vpdaYgjz+3+zPmLlkA/92Yr94\n7lpEIv39Nlj/YWy3mXccXPBAk1f76KOP+OMf/8jbb79NWloaEyZMYNq0aRxxxBEH3Iq+S5cu/OIX\nv+DRRx9l2LBhjdr+iy++yOLFi3n//ffZtGkTw4cP56yzzuLZZ5/lwgsv5Ic//CFVVVXs2bOH+fPn\nR739fUuK1+mvvsCqiOnVYVvUZdy9EtgBdK9vXXdfA/yEIFzWATvc/ZUWqb4ew/p1oVdOJjM+Ur+K\niARee+015s2bR2FhIcOGDWP27Nl8+umn9d6KvqnefPNNLr/8clJTU8nLy2PEiBGUlJQwfPhwHn/8\nce677z4++ugjOnXqFLN9NkWrvaGkmXUlOIrJB7YDL5rZeHd/NsqyE4AJAANiOAxwSkpwCuz389dQ\nVlFFVnrsbh8tIk3QjCOKluLufOtb32LSpEkHzIt2K/pYGTlyJK+//jp//etf+cY3vsGtt97KuHHj\nWnSf0cTrSGUN0D9iul/YFnUZM0sDcoEtDaw7Cvjc3Te5ewXwB+D0aDt398fcvdDdC3v27BmDt7PP\nmII89lRU8cbSTTHdroi0TqNGjaK4uJjNmzcDwVViK1eujHoreoCcnBx27drV6O2feeaZTJs2jerq\najZs2MBbb71FYWEhK1asIC8vjwkTJnD11Vfz3nvv1bvPlhSvI5V5wGAzyycIhMuAusMnTgeuAv4X\nuBiY6e5uZtOB58zsYaAPMBiYC1QDXzKzDsAe4Fyg+bcfbqYvHd6dzllpzFi4gdEFefHevYgkmeOO\nO4577rmHUaNGUV1dTXp6Or/+9a9JTU094Fb0AFdffTXXXnst2dnZzJ0794Arx6699lpuuOEGAPLz\n85k9ezbvvPMOxx9/PGbGww8/TK9evXjyySd5+OGHSU9PJycnh2eeeYZVq1ZF3WdLitut783sy8BP\nCa7SetLdJ5vZRKDE3aeHlwM/A5wIbAUui+jYvxP4FlAJ3Ozufw/b7wMuDdvfA64NO/Trdai3vo/m\ney8sYNaSjZTcOYq0VP30RyQedOv7lnGot76PW5+Ku/8N+FudtrsjXpcBl9Sz7mRgcpT2e4B7Yltp\n040p6M0f31vD3M+3cvqRPRJdjohIwujP6hg466ieZKal6F5gItLuKVRioENGGmcd1ZNXFm2gvY2k\nKZJI+vcWW7H4PBUqMTKmII91O8r4YPWORJci0i5kZWWxZcsWBUuMuDtbtmwhK+vQ7nbVan+nkmxG\nHduL1BRjxsL1nNC/S6LLEWnz+vXrx+rVq9m0SZfzx0pWVhb9+h3a3UEUKjHSpUMGp+Z3Y8bC9dx6\n/jGJLkekzUtPTyc/Pz/RZUgdOv0VQ2MK8vh00xcs27g70aWIiCSEQiWGNMywiLR3CpUYOiw3mxP6\n5WqMFRFptxQqMTa6II/3V+9g3Y49iS5FRCTuFCoxNia8/9crC3U7fBFpfxQqMXZkr04c0bOj+lVE\npF1SqLSAMQV5zPl8K9u+OPjQoCIibYlCpQWcPzSPqmrnnx9vTHQpIiJxpVBpAcf1zaVPbpZOgYlI\nu6NQaQFmxuiCPN5YuonS8spElyMiEjcKlRYyuqA3eyurNcywiLQrCpUWcsqgbnTtkM4MXVosIu2I\nQqWFpKWmcO6xvfnn4g1UVFUnuhwRkbhQqLSgMQV57Cyr5J3PtiS6FBGRuFCotKAzB/egQ0aqrgIT\nkXZDodKCstJTOfuonryycAPV1RqdTkTaPoVKCxtTkMfGXXtZsHp7oksREWlxCpUWVnRML9JSjBkf\n6RSYiLR9CpUWlpudzmlHdGfGwvW46xSYiLRtCpU4GFOQx/ItpSzdoGGGRaRtU6jEweghvTHTMMMi\n0vYpVOKgV+csTuzfRaEiIm2eQiVOxhTksXDtTlZtLU10KSIiLUahEie1wwwv0r3ARKTtUqjEyaAe\nHTm6d45OgYlIm6ZQiaMxBb0pWb6VLbv3JroUEZEWoVCJo9EFeVQ7vLZYp8BEpG1SqMRRQZ/O9O2S\nrTFWRKTNUqjEkZkxpiCPNz/ZzO69GmZYRNoehUqcjSnoTXlVNa8v2ZjoUkREYk6hEmeFg7rRvWOG\nToGJSJukUImz1BRj1LG9mfXxRvZWViW6HBGRmIpbqJjZ+Wa2xMyWmdltUeZnmtkL4fw5ZjYoYt7t\nYfsSMxsT0d7FzF4ys4/NbLGZnRafd3Noxgztze69lbz9qYYZFpG2JS6hYmapwC+BC4AhwOVmNqTO\nYtcA29z9SOAR4MFw3SHAZUABcD7wq3B7AD8D/uHuxwAnAItb+r3EwulH9KBjRiqv6IeQItLGxOtI\n5RRgmbt/5u7lwDTgojrLXAQ8Fb5+CTjXzCxsn+bue939c2AZcIqZ5QJnAU8AuHu5u7eK4RWz0lM5\n55hevLpoA1UaZlhE2pB4hUpfYFXE9OqwLeoy7l4J7AC6N7BuPrAJ+K2ZvWdmj5tZx2g7N7MJZlZi\nZiWbNm2Kxfs5ZGMK8ti8u5x3V25LdCkiIjHTmjvq04CTgCnufiLwBXBAXw2Auz/m7oXuXtizZ894\n1livoqN7kpGaomGGRaRNiVeorAH6R0z3C9uiLmNmaUAusKWBdVcDq919Ttj+EkHItAo5WemcfmR3\nZizSMMMi0nbEK1TmAYPNLN/MMgg63qfXWWY6cFX4+mJgpgffttOBy8Krw/KBwcBcd18PrDKzo8N1\nzgUWtfQbiaUxBXms2rqHxet2JboUEZGYiEuohH0kNwAzCK7QKnb3hWY20cy+Fi72BNDdzJYBtxCe\nynL3hUAxQWD8A/iuu9f8wOM/gKlm9gEwDLg/Hu8nVkYdq2GGRaRtsfZ26qWwsNBLSkoSXUatS379\nNrvKKvnHzWcluhQRkajMbL67FzZm2dbcUd8mjCnI4+P1u1i5RcMMi0jrp1BJsJphhnUKTETaAoVK\ngvXv1oEhh3VWqIhIm6BQSQJjCvKYv3Ibm3ZpmGERad0UKklgzNDeuMOri3Q7fBFp3RQqSeDo3jkM\n7N5Bp8BEpNVTqCSBmmGG3/50MzvLKhJdjohIsylUksSYgt5UVDmzPtYwwyLSeilUksSJ/bvSMyeT\nVzTMsIi0YgqVJJGSYpw3pDezlmykrELDDItI66RQSSJjCvIoLa/izU82J7oUEZFmUagkkdMO705O\nVpquAhORVkuhkkQy0lIYeUwvXlu8gcqq6kSXIyLSZAqVJDOmII9tpRXMW65hhkWk9VGoJJmzj+pJ\nRlqKToGJSKukUEkyHTPTOGtwD15dtEHDDItIq6NQSUKjC/JYs30PH63ZmehSRESapFGhYmapZva6\nmWW2dEESDDOcomGGRaQValSohGPC5zd2eTk03TpmcEp+N4WKiLQ6TQmJ+4ApZjYwPHJJqXm0VHHt\n2ZiCPD7ZuJvPNu1OdCkiIo3WlEB4HPgG8BlQDlQAleGzxNjo2mGGdS8wEWk9mhIq+eHj8IhHzbTE\nWN8u2RzXN1enwESkVWl0qLj7CndfAawiOFJZFdEmLWBMQW8WrNrO+h1liS5FRKRRGh0qZtbZzJ4G\nyoA1wB4ze8rMclusunZuTHgK7NVFOloRkdahKae/fg50BIYC2cBxQIewXVrAkb06cXiPjupXEZFW\nI60Jy54PHO7upeH0UjO7Gvg09mUJBMMMjy7I4/F/fcaO0gpyO6QnuiQRkQY15UilDOhZp60HsDd2\n5UhdYwp6U1nt/PNjHa2ISPJr6iXFr5rZdWZ2gZldB8wAHmuZ0gTghH5d6N05U1eBiUir0JTTX5OB\ntcAVQJ/w9UPAky1Ql4RSUozRQ/J4cf4q9pRXkZ2RmuiSRETq1eh7fxH8on6qu49y9yHh8xOuW+m2\nuDEFeZRVVPPGJ5sSXYqISIOacu+v76BfzyfEqYd3Izc7XafARCTpNaVP5WngupYqROqXnprCucf0\n4p+LN1KhYYZFJIk1JVROAX5mZsvN7F9m9kbNo6WKk31GF+SxY08Fcz/fmuhSRETq1ZSO+t+ED0mA\ns4/qyf9ZMpuC078Nm9fDgAEweTKMG5fo0kREajUqVMKO+iOAye6u36UkQPaL07j/b78gszy8D9iK\nFTBhQvBawSIiSUId9a3FnXfuC5QapaVw552JqUdEJIq4ddSb2flmtsTMlpnZbVHmZ5rZC+H8OWY2\nKGLe7WH7EjMbU2e9VDN7z8xebm5trcLKlVGbvZ52EZFEiEtHfXj67JfABcAQ4HIzG1JnsWuAbe5+\nJPAI8GC47hDgMqCA4P5jvwq3V+MmYHET3kfrNGBA1ObNXXsx57MtcS5GRCS6poTKb4BrgXsIbtny\nRPh4vBHrngIsc/fP3L0cmAZcVGeZi4CnwtcvAeeamYXt09x9r7t/DiwLt4eZ9QO+0sgaWrfJk6FD\nh/2aKjOz+fnIq7n0sXe48ok5LFi1PUHFiYgEDhoqZvZzAHd/yt2fAtJqXofTdcMhmr4Eg3vVWB22\nRV3G3SuBHUD3g6z7U+BWoMEfb5jZBDMrMbOSTZta6a/Sx42Dxx6DgQPBDAYOJO2J33Dncz/mR185\nloVrd/L1X77FtU/NY9HanYmuVkTaqcYcqXyzzvR/15k+LzalNI2ZfRXY6O7zD7asuz/m7oXuXtiz\nZ90bLbci48bB8uVQXR08jxtHVnoq1555OG/cWsR/jjmauZ9v5cs//xfffe5dlm3cleiKRaSdaUyo\nWBOno1kD9I+Y7he2RV3GzNKAXGBLA+ueAXzNzJYTnE4baWbPNqKWNqlTZhrfLTqSf/1wJDeOPJLX\nP97I6Efe4JbiBazY8kWiyxORdqIxoVL3hpEHm45mHjDYzPLNLIOg4316nWWmA1eFry8GZoY3q5wO\nXBZeHZYPDAbmuvvt7t7P3QeF25vp7uMbUUublpudzi2jj+ZfPxzJt888nL99uI5z/2c2t//hQ9Zu\n35Po8kSkjWvMjx/TzKyIfUckdacPei92d680sxsIxl9JBZ5094VmNhEocffpBJ3+z5jZMmArQVAQ\nLlcMLAIqge+Gv5uRBnTrmMHtXz6Wa0bk86vXP+W5OSv5/fzVXHHqAL5TdAS9crISXaKItEF2sDvX\nh6eXGlzI3fNjWFOLKiws9JKSkkSXEXdrtu/h0ZmfUFyymvRU46rTB3HdWUfQtWNGoksTkSRnZvPd\nvbBRy7a34VDaa6jUWL75C372z0/404I1dMxI41sj8rn2zHw6Z6UnujQRSVIKlQa091Cp8cmGXTzy\n2lL+9uF6crPTmXDW4Xzz9EF0zGzKPUZFpD1QqDRAobK/j9bs4JFXl/LPjzfSvWMG159zBOO/NJCs\ndA1bLCIBhUoDFCrRvbtyGw+/spQ3l22md+dMbhg5mEsL+5OR1pSbLohIW6RQaYBCpWH/++kW/ueV\nJZSs2Ea/rtnceO5g/v3EvqSlKlxE2qumhIq+KWQ/px3RnRevO42nvnUK3TpmcOtLH3DeI2/w5wVr\nqK5uX3+AiEjTKVTkAGbG2Uf15M/fPYPHrjyZzLQUbpq2gAt+9i/ef/BX+MCBkJICgwbB1KmJLldE\nkogu9ZF6mRmjC/IYdWxv/vrhOt5/8FcMfvEnWGU4+KdGnxSROnSkIgeVkmJceEIf7nzrGTpU1hlN\nurSUHbfcyj8Xb2B7aXliChSRpKEjFWk0W7UqanvOxnVc81Rw8cNRvTtx8sBuDB/UleGDutGvazbB\nsDgi0h4oVKTxBgwITnkd0N6faRO+RMnyrcxbvo2X31/L83ODYY57d86kcGA3CsOQOSYvR1eSibRh\nChVpvMmTgz6U0tJ9bR06kHL//Xzp8O586fDuAFRVO0s37KoNmfkrtvHXD9cB0DEjlRMHdK0NmWH9\nu+hX/CJtiH6nIk0zdSrceSesXBkcuUye3KhO+jXb91CyfCsly7dRsmIbH6/fiTukphhDDutcGzKF\nA7vSq7PuoCySTPTjxwYoVJLDzrIK3l2xLQyZrSxYtZ2yimBU6IHdO3DywCBkhg/qyhE9Ox3YL9PM\ncBORplOoNEChkpzKK6tZuHZHbciULN/Gli+Cq8m6dkjn5IFdKQxD5vg3/kr6ddcdcBqOxx5TsIi0\nAIVKAxQqrYO78/nmL/YLmc82B8MivzXlW/TdufHAlQYOhOXL41uoSDvQlFBRD6kkJTPj8J6dOLxn\nJ8YO7w/A5t17KVm+jT4PbYq6TvWKldz64vsc1bsTg3vncHTvHA7LzdIlzSJxpFCRVqNHp0zOH5pX\n76XNW7r1ZvbSTbw0f3VtW05mGoN7d+Ko3jn7Hnmd6NkpU2Ej0gIUKtL61HNpc8+f/4R540ax7Yty\nlm7YxdKNu1m6fhdLN+xixsL1TJu378ebXTqkc1SvIGAiA6ebhlcWOSQKFWl9ajrj67n6q2vHDE49\nvDunhr+bgaCPZvPucj7ZsIslG3axdMNulm7YxZ8XrGVXWWXtcj06ZXLUfkc2wam03Ox6hlvWVWgi\n+1FHvbRr7s76nWVByIRHNUs37uaTDbsoLa+qXS6vcxZH5eVwVK9OwXPvHI6Z+ReyvqOr0KTt09Vf\nDVCoSGNUVztrtu8JQiY8qlm6YRfLNu5mb2Xwe5o3p1xNv50HXjSwt28/Nn24hF45WRo5U9oEXf0l\ncohSUoz+3TrQv1sHzj22d217VbWzcmspSzfsou9Dm6Oum75mDSMenAUEp9PycjPJ65xNXm4mh+Vm\n07tzFocqnzHdAAATv0lEQVTlZtU+6zY10pbo/2aRJkhNMfJ7dCS/R8d6r0Ir79OXB/79ONbvLGP9\njjLW7yxj9bZSSlZsZXtpxQHL52Slkdc5i7zcLPJqAic3Mniy6dohveGr1dS3I0lCoSLSXPVchZb1\n0ANcdsqAqKvsKa9iw84y1u0oq/O8h/U797J0wyY27dpL3ZGbM9JSDgye8PmYWX9h4G03k7JnT7Bw\nIgdPU7i1ewoVkeY6yFVo0WRnpDKoR0cG9ehY7zKVVdVs2r03CJwd+wfQ+p1lLFi1nX8sLKO8tm/n\nR/sCpUZpKRv+4/tM2D6I7PQUstNTyUpPDZ4zUslKSyU7IyV8DubVzK9pz8pI3W+97PRUMtNTyExL\niX7UNHXq/iGrkUHbJXXUi7RC7s620grW7yjj2H5dsCj/jh3jqifeoayiirKKKvaUV1FWWcWe8upg\nuqKKqrqHRI1gRm0Y1QRNdnoqv73vEnpt3XDA8l/k9eWdmfPp1jGj9tEpM00/Pm1F1FEv0saZWe0X\ndH19OzZwAE9/65QGt1NRtS9gysqrw9AJp2vCqKKKsorq2lAqK49oC+fvraii59Yo92MDstevrR0Z\ntEZGagrdOmbQtWMG3SPCpuZRt61LhwxSU5oQQjoNlzAKFZHWrp6+HSZPPuiq6akppKemkJNVz487\nm+Ke6OFW3a8ff/ruGWz7opwtX5Sz9Yu9bPminG1flLM1bFu1rZStX5Tv90PUSGbQtUMGXTuk071j\nZhA2nTLo1iEMoU7Bc9cOGfT9+x/octMNmE7DJYRCRaS1a0bfTouoJ9zSHvgvhvXv0qhNlFdWs620\nnC27y4PnL8rZuntvbfjUzPt0027mLQ+m657Be3PKD+kaWQNAaSlbbvoBD2QMJTc7PXh0CJ47Z6XT\nuaYtfMTk90Xt9GhJfSoiEjtx/iKtrnZ27KkIj4CCx5jj+0TtY6rGOH3yq+zYU8GeiqooW9snOz21\nNmA6Z6eFz/sHT00g1YRTzSMrPfXAixYgcXdbiMF/E/2ivgEKFZE2btCgqKfhIsfbKa+sZseeCnaW\nVbBjT/DYGT7vKI1oq51fWTt/997op+hqZKSlMPvRb3LYjgP7mLb1OIyHf/MKmWkp4ZV0qcHrtBQy\n02tepx44v55l01Ls4L9fikG4qaNeRNqvRvQxZaSl0DMnk545mU3efGVVNTvLKvcPoj37h1Pe/dHH\n/MndvJ6/friOvRVV7K2sprIZV99FSjGCsAkv9a4bQr/+0Q/oGeVUIHfe2WJHTAoVEWlbWriPKS28\ncq3BYRLquSIvZeAA3r3rvNrpyqpqyquq2VtRzd7KavZWBmETTFfta2twfnVtSNVdvnuUS7yB4HNp\nIQoVEWl7xo1LbKd4I6/IS0tNIS01hQ4tNYzPj6OHGwOi3/EhFnQLVRGRWBs3Lui3GDgwuB564MDE\ndNJPnhyEWaRGXm7eXHELFTM738yWmNkyM7styvxMM3shnD/HzAZFzLs9bF9iZmPCtv5mNsvMFpnZ\nQjO7KV7vRUTkoMaNCy4MqK4OnhNx5JSAcIvL6S8zSwV+CZwHrAbmmdl0d18Usdg1wDZ3P9LMLgMe\nBC41syHAZUAB0Ad4zcyOAiqB77v7u2aWA8w3s1frbFNEpH2L86nAeB2pnAIsc/fP3L0cmAZcVGeZ\ni4CnwtcvAedacK3cRcA0d9/r7p8Dy4BT3H2du78L4O67gMVA3zi8FxERqUe8QqUvsCpiejUHBkDt\nMu5eCewAujdm3fBU2YnAnBjWLCIiTdTqO+rNrBPwe+Bmd99ZzzITzKzEzEo2bYp+/biIiBy6eIXK\nGqB/xHS/sC3qMmaWBuQCWxpa18zSCQJlqrv/ob6du/tj7l7o7oU9e/Y8xLciIiL1iVeozAMGm1m+\nmWUQdLxPr7PMdOCq8PXFwEwP7iEzHbgsvDosHxgMzA37W54AFrv7w3F5FyIi0qC4XP3l7pVmdgMw\nA0gFnnT3hWY2EShx9+kEAfGMmS0DthIED+FyxcAigiu+vuvuVWY2ArgS+NDMFoS7usPd/xaP9yQi\nIgfSDSVFRKRBTbmhZKvvqBcRkeShUBERkZhRqIiISMwoVEREJGYUKiIiEjMKFRERiRmFioiIxIxC\nRUREYkahIiIiMaNQERGRmFGoiIhIzChUREQkZhQqIiISMwoVERGJGYWKiIjEjEJFRERiRqEiIiIx\no1AREZGYUaiIiEjMKFRERCRmFCoiIhIzChUREYkZhYqIiMSMQkVERGJGoSIiIjGjUBERkZhRqIiI\nSMwoVEREJGYUKiIiEjMKFRERiRmFykE89NZDzPp81n5tsz6fxUNvPZSgikREkpdC5SCG9xnO2JfG\n1gbLrM9nMfalsQzvMzzBlYmIJJ+0RBeQ7Iryiyi+uJixz32F63udwJSNH1B83FUUbVoGO9ZDZifI\nzIGMnIjXnYJHSuwy+6G3HmJ4n+EU5RfVts36fBbz1s7j1jNujdl+REQOhUKlEYryi7g+tTOT1r7D\nXWRR9O5UYOrBV0zvGIRMZhgymTn7QqduGNXO7xS27d9ec8RUfHExRflFtUdMxRcXt/j7r5EswZYs\ndYgku0T8W1GoNMKsz2cxJbWKu866iyklUyj6+jMUHXYS7N0F5bth7+6I17v2by8Pp/fuDtq2r4K9\nO/fNr9rbqBqK0rIoTk1n7DPncX12HlPKNlLc4ySK3noU5j4BadmQHj7SsiC9A6Rn7d9eO6/mdXaw\nTHqHfeuk1v+/RDIEWzLVkSzhpjqSr45kqAES829FoXIQkf8RivKLKBpUtN/0Iass3xdGtaFUE0b7\ntxft3cX1q2YzadMC7upyNEVZ3aB0C1SUQeUeqNgTvK4oheqK5tWTkhYRRPuHUlF6NsXdjmfs1Au4\nvsdQpmxeSPHA8yha8AK8Nw1wcA83VPP6YM81izdm2eC5CCjufBRjnz2f6zsPYsrO5RQfdjpFcx6H\neb8N3kNKGqSkQ0pqxHRaMJ2avv90Q8unph24frjs8LROjC3+d4rPfYiivqcya+08xr72nxSP+m9Y\n/2HzPv9mGJ7VnbHF/4fiUf+9r45Xv0/xmJ/C5mVgFj5SggcRr2sftv9zg8uEr+vWkSRhnwx1NKsG\nd6iugurKiEfNdEWd6YhHVWU961RSVF1B8UnXM3baRVx/+GimrJwdu++uephH/sNuBwoLC72kpKTR\nyyfLXxw1+x370liuL7yeKSVTGv6fo7oqDJk9EYGzByrD0KkJn8qy6PMi16mdF7y+e9dnTCrbwF0Z\nPZiY2Sv8gjEwwmeL8tzQPGvkNsLthK/v3vkpk3Z9zl2dBjKx04B9/5iqGvgHGPmIxX8TKhnLHq4n\nnSlUUEw2RQn4Wy0xddQNHmOWVzC2egfXp3RkSvUXFKd2oyg1a9/yHPhy//+ujWjbrz1626yqUsaW\nreP69C5MqdhOcVYfitI6Hrhe1H3WV0Mjp8O2WRU7GbvrM67P6smUsk0Ud+hHUUp29ACoqgCvoqXc\nTRmTrJy7zrqLiUUTm7y+mc1398JGLRuvUDGz84GfAanA4+7+QJ35mcDTwMnAFuBSd18ezrsduAao\nAm509xmN2WY0TQ2VZFH3iKnudLzraFSwJXMd7uDVTQuhmmVqlw+m737/t0xaNI27hlzKxKHjWu5N\nH8TdHz7LpMXF3HXsJUwcclnw/mreJ+Fz7SPKdCyWwbl75etMWvMWd/U9g4n9ztz3edeqe4TalLaI\n9oO03b3uHSatn8ddvQuZeNipEcs0oo6mTtezzN2bFjBp8wfc1XMYE/NOOcjRbwNH1ylpEUfY9cxP\niX4EPmttCWNn3MT1J1zJlA+nNuvfbFNCBXdv8QfBl/6nwOFABvA+MKTOMt8Bfh2+vgx4IXw9JFw+\nE8gPt5PamG1Ge5x88sneGj345oM+87OZ+7XN/GymP/jmg3GrYeZnM73HQz1q66g73d7qiNz3XTPv\nSlgNqiM560imGg713wpQ4o39vm/sgofyAE4DZkRM3w7cXmeZGcBp4es0YDPB8eR+y9Ys15htRnu0\n1lBJBskQbMlUR7KEm+pIvjqSoQb32P1bScZQuZjg9FTN9JXAo3WW+QjoFzH9KdADeBQYH9H+RLi9\ng24zYt4EoAQoGTBgQJM+TJH6JEu4qY7kqyMZaoilpoRKXPpUzOxi4Hx3vzacvhI41d1viFjmo3CZ\n1eH0p8CpwL3AO+7+bNj+BPD3cLUGtxlNa+1TERFJlKb0qcTrNi1rgP4R0/3CtqjLmFkakEvQYV/f\nuo3ZpoiIxFG8QmUeMNjM8s0sg6AjfnqdZaYDV4WvLwZmhodd04HLzCzTzPKBwcDcRm5TRETiKC4X\n1Lt7pZndQNDJngo86e4LzWwiwbm66QR9Jc+Y2TJgK0FIEC5XDCwCKoHvugcXdEfbZjzej4iIRKcf\nP4qISIOSsU9FRETagXZ3pGJmm4AVzVy9B8HvZxItGepIhhpAddSlOvaXDHUkQw1waHUMdPeejVmw\n3YXKoTCzksYeArb1OpKhBtWhOlpDHclQQzzr0OkvERGJGYWKiIjEjEKlaR5LdAGhZKgjGWoA1VGX\n6thfMtSRDDVAnOpQn4qIiMSMjlRERCRmFCqNYGZPmtnG8KaXiaqhv5nNMrNFZrbQzG5KUB1ZZjbX\nzN4P67gvEXVE1JNqZu+Z2csJrGG5mX1oZgvMLCG/rDWzLmb2kpl9bGaLzey0BNRwdPgZ1Dx2mtnN\n8a4jrOV74f+fH5nZ82aWdfC1WqSOm8IaFsbzs4j2nWVm3czsVTP7JHzu2hL7Vqg0zu+A8xNcQyXw\nfXcfAnwJ+K6ZDUlAHXuBke5+AjAMON/MvpSAOmrcBCxO4P5rFLn7sAReOvoz4B/ufgxwAgn4TNx9\nSfgZDCMYwbUU+GO86zCzvsCNQKG7DyW4jdNlCahjKPBt4BSC/yZfNbMj47T733Hgd9ZtwD/dfTDw\nz3A65hQqjeDubxDcjyyRNaxz93fD17sIvjT6JqAOd/fd4WR6+EhIx5yZ9QO+AjyeiP0nCzPLBc4i\nuH8e7l7u7tsTWxXnAp+6e3N/aHyo0oDs8I7nHYC1CajhWGCOu5e6eyUwG/j3eOy4nu+si4CnwtdP\nAV9viX0rVFohMxsEnAjMSdD+U81sAbAReNXdE1IH8FPgVqA6Qfuv4cArZjbfzCYkYP/5wCbgt+Gp\nwMfNrGMC6oh0GfB8Inbs7muAnwArgXXADnd/JQGlfAScaWbdzawD8GX2H64j3nq7+7rw9Xqgd0vs\nRKHSyphZJ+D3wM3uvjMRNbh7VXiKox9wSniYH1dm9lVgo7vPj/e+oxjh7icBFxCcljwrzvtPA04C\nprj7icAXtNCpjcYIh6L4GvBigvbfleCv8nygD9DRzMbHuw53Xww8CLwC/ANYAFTFu45owmFFWuQM\ng0KlFTGzdIJAmeruf0h0PeEpllkkpr/pDOBrZrYcmAaMNLNnE1BHzV/GuPtGgj6EU+JcwmpgdcQR\n40sEIZMoFwDvuvuGBO1/FPC5u29y9wrgD8DpiSjE3Z9w95Pd/SxgG7A0EXWENpjZYQDh88aW2IlC\npZUwMyM4Z77Y3R9OYB09zaxL+DobOA/4ON51uPvt7t7P3QcRnGqZ6e5x/2vUzDqaWU7Na2A0wWmP\nuHH39cAqMzs6bDqXYPyhRLmcBJ36Cq0EvmRmHcJ/N+eSoIs5zKxX+DyAoD/luUTUEYocCPEq4M8t\nsZO4DNLV2pnZ88A5QA8zWw3c4+5PxLmMM4ArgQ/D/gyAO9z9b3Gu4zDgKTNLJfijpNjdE3Y5bxLo\nDfwx+O4iDXjO3f+RgDr+A5gannr6DLg6ATXUBOt5wP9NxP4B3H2Omb0EvEtw1eR7JO5X7b83s+5A\nBcEAg3G5gCLadxbwAFBsZtcQ3Kl9bIvsW7+oFxGRWNHpLxERiRmFioiIxIxCRUREYkahIiIiMaNQ\nERGRmFGoiLQSZuZxvCGhSLMoVESaKbzl/R4z2x3xeDTRdYkkkn78KHJoLnT31xJdhEiy0JGKSIyZ\n2TfN7C0ze9TMdoQDZ50bMb+PmU03s61mtszMvh0xL9XM7jCzT81sV3jn48g7244KB1nabma/DG9D\ngpkdaWazw/1tNrMX4viWRWrpSEWkZZxKcGPHHgT3fPqDmeW7+1aCG2B+RHAH3WOAV83sU3efCdxC\ncO+sLxPcfPB4gsGuanwVGA50BuYDfyG4A+4kgrvhFgEZQKIGC5N2TrdpEWmm8A7JPQjuL1XjPwnu\n83Q/0De8xThmNhf4BfA6sBzoEg62hpn9F3CYu3/TzJYAt7r7ATf7MzMHznT3N8PpYoK7AT9gZk8D\nZcBEd1/dAm9XpFF0+kvk0Hzd3btEPH4Ttq/x/f9iW0FwZNIH2FoTKBHzakbx7A982sD+1ke8LgU6\nha9vBQyYG46H/q1mvh+RQ6JQEWkZfWv6O0IDCIa0XQt0q7ldfsS8NeHrVcARTd2Zu69392+7ex+C\nOwT/SpcfSyIoVERaRi/gRjNLN7NLCMYr/5u7rwLeBv7LzLLM7HjgGqBmgLHHgUlmNtgCx4e3Tm+Q\nmV1iZv3CyW0Eo/olephlaYfUUS9yaP5iZpFDxL5KMPjRHGAwsBnYAFzs7lvCZS4Hfk1w1LKNYHye\nmsuSHwYyCTrdexAMgPZvjahjOPBTM8sN93eTu392KG9MpDnUUS8SY2b2TeBadx+R6FpE4k2nv0RE\nJGYUKiIiEjM6/SUiIjGjIxUREYkZhYqIiMSMQkVERGJGoSIiIjGjUBERkZhRqIiISMz8f5n8vr9r\njNq1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7880370f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting Losses\n",
    "epochs_arr = [i for i in range(1,11)]\n",
    "\n",
    "train_arr = np.array(train_arr).flatten()\n",
    "test_arr = np.array(test_arr).flatten()\n",
    "\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.plot(epochs_arr,train_arr,label=\"Train Loss\")\n",
    "plt.plot(epochs_arr,test_arr,label=\"Test Loss\")\n",
    "plt.legend()\n",
    "plt.plot(epochs_arr,train_arr, 'ro')\n",
    "plt.plot(epochs_arr,test_arr, 'gx')\n",
    "fig2.suptitle('Error vs Epoch', fontsize=20)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Error', fontsize=12)\n",
    "plt.xticks(range(1,11))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for plotting Training and Test error after every epoch\n",
    "#### In train and test fucntion we need to return total_loss for that epoch. In the outer loop for epoch, we can collect those two results and store them in an array (or not) and then plot it against number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best settings for vanilla SGD:\n",
    "#### Learning rate = 0.01\n",
    "#### Momentum = 0.9\n",
    "#### Nesterov = True\n",
    "#### Batch_size = 64\n",
    "#### Test Accuracy ~ 96%\n",
    "#### Explanation: Learning rate should be high enough such that model converges fast and should be low enough to not overshoot from the solution. And momentum makes sure that we use previous gradient to find direction in next iteration.\n",
    "\n",
    "### Best settings for Adagrad:\n",
    "#### Learning rate = 0.01\n",
    "#### Batch_size = 64\n",
    "#### Test Accuracy ~ 96%\n",
    "#### Same reason as above. Tried using lr_scheduler in torch.optim package to optimize the learning rate, but found no difference in accuracy. \n",
    "\n",
    "### Best settings for Adam:\n",
    "#### Learning rate = 0.001\n",
    "#### Batch_size = 64\n",
    "#### Test Accuracy ~ 96%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
